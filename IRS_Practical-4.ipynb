{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical - 4\n",
    "\n",
    "\n",
    "Name: Rohan Pillai<br>\n",
    "Roll Number: 15BIT049<br>\n",
    "Batch: A3<br>\n",
    "Course Code: IT702<br>\n",
    "Course Name: Information Retrieval Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['new', 'york', 'time', 'post', 'lo', 'angel']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#doc = ['My name is Rohan Pillai. His name is XYZ','It is raining today','I have a lot of homework to do','Today we have to complete Practical 4']\n",
    "#query = 'is it raining today'\n",
    "doc = ['new york times','new york post','los angeles times']\n",
    "query = 'new new times'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmed_words = []\n",
    "\n",
    "for i in doc:\n",
    "    i=i.lower();\n",
    "    #Removes Punctuations\n",
    "    translation_table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    #Remove \\n for new line to space\n",
    "    i.replace('\\n', ' ')\n",
    "    i = i.translate(translation_table)\n",
    "\n",
    "    #Tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    cloud_input = ''\n",
    "\n",
    "    #Stopwords Removal\n",
    "    without_stopwords = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            without_stopwords.append(w)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    #Stemming    \n",
    "    for w in without_stopwords:\n",
    "        if ps.stem(w) not in stemmed_words:\n",
    "            stemmed_words.append(ps.stem(w))\n",
    "            \n",
    "        \n",
    "#stemmed_words.sort()\n",
    "print(\"Vocabulary:\" , stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['new', 'york', 'time', 'post', 'lo', 'angel']\n",
      "TF Vector:\n",
      "[1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 1, 1]\n",
      "[2, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "def preprocessing(i):\n",
    "    stemmed_words = ''\n",
    "    i=i.lower()\n",
    "    #Removes Punctuations\n",
    "    translation_table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    #Remove \\n for new line to space\n",
    "    i.replace('\\n', ' ')\n",
    "    i = i.translate(translation_table)\n",
    "\n",
    "    #Tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    cloud_input = ''\n",
    "\n",
    "    #Stopwords Removal\n",
    "    without_stopwords = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            without_stopwords.append(w)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    #Stemming    \n",
    "    for w in without_stopwords:\n",
    "        stemmed_words+=(ps.stem(w))\n",
    "        stemmed_words+=\" \"\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "doc_term_matrix = []\n",
    "\n",
    "print(\"Vocabulary: \", list(stemmed_words))\n",
    "doc.append(query)\n",
    "print(\"TF Vector:\")\n",
    "for doc1 in doc:\n",
    "    a=preprocessing(doc1.lower())\n",
    "    tf_vector = [tf(word,a) for word in stemmed_words]\n",
    "    tf_vector_string = ','.join(format(freq,'d') for freq in tf_vector)\n",
    "    tf_vector_arr = list(tf_vector_string.split(','))\n",
    "    for i in range(0,len(tf_vector_arr)):\n",
    "        tf_vector_arr[i]=int(tf_vector_arr[i])\n",
    "    print(tf_vector_arr)\n",
    "    doc_term_matrix.append(tf_vector_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Array:  [2, 2, 2, 1, 1, 1]\n",
      "IDF Vector:  [1.791759469228055, 1.791759469228055, 1.791759469228055, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003]\n",
      "\n",
      "TF-IDF Vector: \n",
      " [[1.79175947 1.79175947 1.79175947 0.         0.         0.        ]\n",
      " [1.79175947 1.79175947 0.         1.60943791 0.         0.        ]\n",
      " [0.         0.         1.79175947 0.         1.60943791 1.60943791]\n",
      " [3.58351894 0.         1.79175947 0.         0.         0.        ]]\n",
      "\n",
      "Query Vector:  [3.58351893845611, 0.0, 1.791759469228055, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "count_arr=[]\n",
    "doc_term_matrix1 = []\n",
    "for i in range(0, len(doc_term_matrix[0])):\n",
    "    count=0\n",
    "    for j in range(0, len(doc_term_matrix)-1):\n",
    "         if(doc_term_matrix[j][i]>0):\n",
    "                count+=1\n",
    "    count_arr.append(count)\n",
    "print(\"Count Array: \", count_arr)\n",
    "\n",
    "def idf(word, doc):\n",
    "    noOfSamples = len(doc)\n",
    "    df = count_arr[stemmed_words.index(word)]\n",
    "    return np.log(noOfSamples/1+df)\n",
    "\n",
    "my_idf_vector = [idf(word,doc) for word in stemmed_words]\n",
    "print(\"IDF Vector: \", my_idf_vector)\n",
    "\n",
    "for i in range(0,len(doc_term_matrix)):\n",
    "    doc_term_matrix1.append([a*b for a,b in zip(doc_term_matrix[i],my_idf_vector)])\n",
    "    \n",
    "print(\"\")\n",
    "print(\"TF-IDF Vector: \\n\", np.matrix(doc_term_matrix1))\n",
    "\n",
    "query = doc_term_matrix1[-1]\n",
    "print(\"\\nQuery Vector: \", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.58351893845611, 0.0, 1.791759469228055, 0.0, 0.0, 0.0]\n",
      "\n",
      "Length:\n",
      "Doc  1 :  3.1034184356456356\n",
      "Doc  2 :  3.0018484946973985\n",
      "Doc  3 :  2.896719313901309\n",
      "Query:  4.006495972522873 \n",
      "\n",
      "Cosine Similarity between Document and Query:  [3.1034184356456356, 2.1389500511031128, 1.1082889461059393]\n",
      "\n",
      "Most similar document is Document  1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(query)\n",
    "\n",
    "def lengthOfVec(list):\n",
    "    sum = 0\n",
    "    for i in list:\n",
    "        sum+=i**2\n",
    "    return math.sqrt(sum)\n",
    "\n",
    "print(\"\\nLength:\")\n",
    "for i in doc_term_matrix1:\n",
    "    indexDoc = doc_term_matrix1.index(i)\n",
    "    if indexDoc==len(doc_term_matrix1)-1:\n",
    "        print(\"Query: \", lengthOfVec(i),\"\\n\")\n",
    "    else:\n",
    "        print(\"Doc \", indexDoc+1, \": \", lengthOfVec(i))\n",
    "        \n",
    "\n",
    "final_result = []\n",
    "indexDoc = 0\n",
    "for i in doc_term_matrix1:    \n",
    "    if indexDoc==len(doc_term_matrix1)-1:\n",
    "        break\n",
    "    else:\n",
    "        a1 = [a*b for a,b in zip(i,query)]\n",
    "        final_result.append(sum(a1)/lengthOfVec(i))\n",
    "    indexDoc+=1\n",
    "\n",
    "print(\"Cosine Similarity between Document and Query: \",final_result)\n",
    "print(\"\\nMost similar document is Document \",final_result.index(max(final_result))+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
